{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Performance with MKL BLAS on MXNet 1.6 Deep Learning Containers\n",
    "\n",
    "Inference speed and performance is often times one of the most crucial factors for deciding to deploy a model in a production environment. Small increases in latency can be costly, so every bit of performance boost that can be had can help the overall costs. Because of our obessesion for customers, the MXNet team has brought a solution to improve the latency for inference using MXNet 1.6 Deep learning containers. In this post, we dicuss the improvement that was made to the MXNet 1.6.0 DLC version to make use of highly optimized matrix operators. The enhancement comes in the form of compiling MXNet with a dependency on Intel MKL BLAS instead of the default, oneDNN. As one will see, the performance boost can be up to 30% in latency reduction, making this a worthwhile option for our customers to implement in their production environment. To describe and show the enhancements in more detail, we will use the MNIST dataset to briefly give context of performing inference on Amazon SageMaker. Then, we will discuss the differences between the MKL BLAS and oneDNN libraries. Lastly, we will show how to implement the enhancement in your environment so that you can take advantage of the performance boost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: MXNet 1.7+ Deep Learning Containers have this enhancement as a default, so this solution applies to customers who don't want to change the MXNet version from 1.6.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker makes it really easy to deploy, host and maintain models. As part of that, choosing what framework or deep learning container to use is also a matter of setting a parameter. For the remainder of this post, we will use the MNIST dataset to quickly give examples for context and to show the performance difference between the MKL BLAS library and oneDNN library. But first, here is a brief example of how to perform inference in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First we define a few variables that are needed to perform operations in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gluonnlp --quiet\n",
    "!pip install bert --quiet\n",
    "!pip install mxnet --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gluoncv --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel, MXNetPredictor\n",
    "import pandas as pd\n",
    "from sagemaker import utils\n",
    "import boto3\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import mxnet as mx\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import gluonnlp as nlp\n",
    "import cv2\n",
    "from transform import BERTDatasetTransform\n",
    "from gluonnlp.calibration import BertLayerCollector\n",
    "from utils import test_bert, deploy_bert, deploy_model\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Bucket location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/mxnet-mnist-example/artifacts'.format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "test_data_location = 'sagemaker-sample-data-{}'.format(region)\n",
    "\n",
    "# containers that will be used for comparision\n",
    "oneDnn_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.7\"\n",
    "mkl_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dot operator using AWS Deep Learning Containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data \n",
    "\n",
    "This dataset contains 10,000 images that 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding=\"ISO-8859-1\", names=[\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an inference Endpoint\n",
    "\n",
    "We use the ``MXNet model`` object to load model data and deploy an ``MXNetPredictor``. This creates a Sagemaker **Endpoint** -- a hosted prediction service that we can use to perform inference. \n",
    "\n",
    "The arguments to the ``deploy`` function allow us to set the number and type of instances that will be used for the Endpoint. Here we will deploy the model to a single ``ml.m4.xlarge`` instance. By not setting the ``image`` parameter, ``MXNetModel`` uses the default deep learning container image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKL BLAS vs oneDNN\n",
    "\n",
    "The MKL BLAS and oneDNN libraries of math routines that are used to perform mathmatical operations on data. You can think of these as low level instructions that progamming languages use to perform computations. In the case of MXNet, it uses these libraries to for its operations such as dot products and other computationally expensive operations. MKL BLAS as version implemented by Intel, that uses highly optimized operators for CPU. These operators like the ``dot`` operator, are much faster than the operators found on the default library, oneDNN. So in order to take advantage of the speed boost, the MXNet team packaged the the MXNet 1.6 version with Intel's MKL BLAS library in a deep learning container available for use. In the diagram below, you can see the changes that are made between the different math libraries and MXNet versions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "In order to see the performance increase, we describe now how to implement the MKL BLAS and oneDNN based deep learning containers using SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oneDNN \n",
    "\n",
    "Notice the parameter ``image`` is set to a uri. This is the deep learning container uri, and the ``v3.7`` specifies a version of this MXNet 1.6 container that is compiled with oneDNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying...\n",
      "-----------------!\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.7\"\n",
    "onednn_predictor = deploy_bert(sagemaker_session, ecr_image, 'ml.m4.xlarge', \"1.6.0\", role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 55.409968686103824 Std: 0.12656422301418305 with 10 loops\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[55.78212642669678,\n",
       " 55.4226975440979,\n",
       " 55.36242318153381,\n",
       " 55.377811670303345,\n",
       " 55.33304786682129,\n",
       " 55.36034035682678,\n",
       " 55.37596607208252,\n",
       " 55.37017631530762,\n",
       " 55.3291597366333,\n",
       " 55.38593769073486]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times = inference(tweets.text[:100], onednn_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MKL BLAS\n",
    "\n",
    "To specify the deep learning container with MKL BLAS, you change the version identifier in the image uri to ``v3.8``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying...\n",
      "-----------------!\n",
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.8\"\n",
    "mkl_predictor = deploy_bert(sagemaker_session, ecr_image, 'ml.m4.xlarge', \"1.6.0\", role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 48.03284316062927 Std: 0.09309706624115292 with 10 loops\n"
     ]
    }
   ],
   "source": [
    "times = inference(tweets.text[:100], mkl_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying...\n",
      "-------------------!\n",
      "Endpoint Name: bert-1605561706-78b3\n"
     ]
    }
   ],
   "source": [
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.8\"\n",
    "mklq_predictor = deploy_bert(\"bert_sst_quantized.tar.gz\", sagemaker_session, ecr_image, 'ml.m4.xlarge', \"1.6.0\", role, script=\"bert_inference_quantized.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 59.52104277610779 Std: 0.1776845656988009 with 10 loops\n"
     ]
    }
   ],
   "source": [
    "times = inference(tweets.text[:100], mklq_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying...\n",
      "-------------------!\n",
      "Endpoint Name: ssd-test-1606158680-a1ac\n"
     ]
    }
   ],
   "source": [
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-gpu-py36-cu101-ubuntu16.04-v3.7\"\n",
    "ssd_predictor = deploy_model(\"models/ssd_512_resnet50_v1_voc.tar.gz\", \"ssd-test\", sagemaker_session, ecr_image, 'ml.p2.8xlarge', \"1.6.0\", role, script=\"ssd_inference-batch50.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 48.4411301612854 Std: 1.2594568489112994 with 10 loops\n"
     ]
    }
   ],
   "source": [
    "#ssd_predictor = MXNetPredictor(\"ssd-test-1605820634-af4b\")\n",
    "img = cv2.imread(\"street_small.jpg\")\n",
    "times_ssd = inference(img, ssd_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying...\n",
      "------------------!\n",
      "Endpoint Name: ssd-test-mkl-1606159796-52ae\n"
     ]
    }
   ],
   "source": [
    "ecr_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/mxnet-inference:1.6.0-gpu-py36-cu101-ubuntu16.04-v3.8\"\n",
    "mkl_ssd_predictor = deploy_model(\"models/ssd_512_resnet50_v1_voc.tar.gz\", \"ssd-test-mkl\", sagemaker_session, ecr_image, 'ml.p2.8xlarge', \"1.6.0\", role, script=\"ssd_inference-batch50.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg: 46.63021864891052 Std: 0.454994058507219 with 10 loops\n"
     ]
    }
   ],
   "source": [
    "times_ssd_mkl = inference(img, mkl_ssd_predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "oneDNN_predictor.delete_endpoint()\n",
    "mklblas_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
