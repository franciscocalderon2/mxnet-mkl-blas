{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Performance with MKL BLAS on MXNet 1.6 Deep Learning Containers\n",
    "\n",
    "Inference speed and performance is often times one of the most crucial factors for deciding to deploy a model in a production environment. Small increases in latency can be costly, so every bit of performance boost that can be had can help the overall costs. Because of our obessesion for customers, the MXNet team has brought a solution to improve the latency for inference using MXNet 1.6 Deep learning containers. In this post, we dicuss the improvement that was made to the MXNet 1.6.0 DLC version to make use of highly optimized matrix operators. The enhancement comes in the form of compiling MXNet with a dependency on Intel MKL BLAS instead of the default, oneDNN. As one will see, the performance boost can be up to 30% in latency reduction, making this a worthwhile option for our customers to implement in their production environment. To describe and show the enhancements in more detail, we will use the MNIST dataset to briefly give context of performing inference on Amazon SageMaker. Then, we will discuss the differences between the MKL BLAS and oneDNN libraries. Lastly, we will show how to implement the enhancement in your environment so that you can take advantage of the performance boost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: MXNet 1.7+ Deep Learning Containers have this enhancement as a default, so this solution applies to customers who don't want to change the MXNet version from 1.6.0**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference on Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker makes it really easy to deploy, host and maintain models. As part of that, choosing what framework or deep learning container to use is also a matter of setting a parameter. For the remainder of this post, we will use the MNIST dataset to quickly give examples for context and to show the performance difference between the MKL BLAS library and oneDNN library. But first, here is a brief example of how to perform inference in SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First we define a few variables that are needed to perform operations in SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.mxnet.model import MXNetModel, MXNetPredictor\n",
    "import boto3\n",
    "import gzip\n",
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "# S3 bucket for saving code and model artifacts.\n",
    "# Feel free to specify a different bucket here if you wish.\n",
    "bucket = Session().default_bucket()\n",
    "\n",
    "# Bucket location where results of model training are saved.\n",
    "model_artifacts_location = 's3://{}/mxnet-mnist-example/artifacts'.format(bucket)\n",
    "\n",
    "# IAM execution role that gives SageMaker access to resources in your AWS account.\n",
    "# We can use the SageMaker Python SDK to get the role from our notebook environment. \n",
    "role = get_execution_role()\n",
    "sagemaker_session = Session()\n",
    "region = boto3.Session().region_name\n",
    "test_data_location = 'sagemaker-sample-data-{}'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the MNIST test dataset\n",
    "\n",
    "This dataset contains 10,000 images that 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.download_data('test_test', test_data_location, key_prefix=\"mxnet/mnist/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility Function\n",
    "These functions will help to load data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with gzip.open(find_file(path, \"labels.gz\")) as flbl:\n",
    "        struct.unpack(\">II\", flbl.read(8))\n",
    "        labels = np.frombuffer(flbl.read(), dtype=np.int8)\n",
    "    with gzip.open(find_file(path, \"images.gz\")) as fimg:\n",
    "        _, _, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        images = np.frombuffer(fimg.read(), dtype=np.uint8).reshape(len(labels), rows, cols)\n",
    "        images = images.reshape(images.shape[0], 1, 28, 28).astype(np.float32) / 255\n",
    "    return labels, images\n",
    "\n",
    "\n",
    "def find_file(root_path, file_name):\n",
    "    for root, dirs, files in os.walk(root_path):\n",
    "        if file_name in files:\n",
    "            return os.path.join(root, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, images = load_data('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an inference Endpoint\n",
    "\n",
    "We use the ``MXNet model`` object to load model data and deploy an ``MXNetPredictor``. This creates a Sagemaker **Endpoint** -- a hosted prediction service that we can use to perform inference. \n",
    "\n",
    "The arguments to the ``deploy`` function allow us to set the number and type of instances that will be used for the Endpoint. Here we will deploy the model to a single ``ml.m4.xlarge`` instance. By not setting the ``image`` parameter, ``MXNetModel`` uses the default deep learning container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MXNetModel(\n",
    "    model_data=\"s3://sagemaker-us-west-2-738657245266/mxnet-mnist-example/artifacts/mxnet-training-2020-10-21-21-49-55-741/output/model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\"\n",
    ")\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                 instance_type='ml.m4.xlarge',\n",
    "                endpoint_name=\"default-image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 s, sys: 573 ms, total: 30.7 s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for image in images:\n",
    "    predictor.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MKL BLAS vs oneDNN\n",
    "\n",
    "The MKL BLAS and oneDNN libraries of math routines that are used to perform mathmatical operations on data. You can think of these as low level instructions that progamming languages use to perform computations. In the case of MXNet, it uses these libraries to for its operations such as dot products and other computationally expensive operations. MKL BLAS as version implemented by Intel, that uses highly optimized operators for CPU. These operators like the ``dot`` operator, are much faster than the operators found on the default library, oneDNN. So in order to take advantage of the speed boost, the MXNet team packaged the the MXNet 1.6 version with Intel's MKL BLAS library in a deep learning container available for use. In the diagram below, you can see the changes that are made between the different math libraries and MXNet versions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance\n",
    "\n",
    "In order to see the performance increase, we describe now how to implement the MKL BLAS and oneDNN based deep learning containers using SageMaker. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### oneDNN \n",
    "\n",
    "Notice the parameter ``image`` is set to a uri. This is the deep learning container uri, and the ``v3.7`` specifies a version of this MXNet 1.6 container that is compiled with oneDNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------"
     ]
    }
   ],
   "source": [
    "oneDNN_model = MXNetModel(\n",
    "    model_data=\"s3://sagemaker-us-west-2-738657245266/mxnet-mnist-example/artifacts/mxnet-training-2020-10-21-21-49-55-741/output/model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    image=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.7\"\n",
    ")\n",
    "\n",
    "oneDNN_predictor = oneDNN_model.deploy(initial_instance_count=1,\n",
    "                 instance_type='ml.m4.xlarge',\n",
    "                endpoint_name=\"oneDNN-image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 s, sys: 530 ms, total: 30.1 s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for image in images:\n",
    "    oneDNN_predictor.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MKL BLAS\n",
    "\n",
    "To specify the deep learning container with MKL BLAS, you change the version identifier in the image uri to ``v3.8``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "mklblas_model = MXNetModel(\n",
    "    model_data=\"s3://sagemaker-us-west-2-738657245266/mxnet-mnist-example/artifacts/mxnet-training-2020-10-21-21-49-55-741/output/model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"1.6.0\",\n",
    "    py_version=\"py3\",\n",
    "    image=\"763104351884.dkr.ecr.us-west-2.amazonaws.com/mxnet-inference:1.6.0-cpu-py36-ubuntu16.04-v3.8\"\n",
    ")\n",
    "\n",
    "mklblas_predictor = mklblas_model.deploy(initial_instance_count=1,\n",
    "                 instance_type='ml.m4.xlarge',\n",
    "                endpoint_name=\"mklblas-image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.2 s, sys: 564 ms, total: 29.7 s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for image in images:\n",
    "    mklblas_predictor.predict(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Delete the Endpoint\n",
    "\n",
    "After you have finished with this example, remember to delete the prediction endpoint to release the instance(s) associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "oneDNN_predictor.delete_endpoint()\n",
    "mklblas_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
